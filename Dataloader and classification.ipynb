{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68309ed9-e60f-4362-a15f-11e559568d9c",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77d956e1-1a56-484a-a9dd-70c6f449d18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>Y</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>30.4</td>\n",
       "      <td>85.00</td>\n",
       "      <td>198</td>\n",
       "      <td>115.6</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.3438</td>\n",
       "      <td>80</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>111.00</td>\n",
       "      <td>214</td>\n",
       "      <td>147.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.9416</td>\n",
       "      <td>91</td>\n",
       "      <td>144</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>103.67</td>\n",
       "      <td>210</td>\n",
       "      <td>85.2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.1070</td>\n",
       "      <td>124</td>\n",
       "      <td>245</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>73.00</td>\n",
       "      <td>191</td>\n",
       "      <td>119.4</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.8520</td>\n",
       "      <td>92</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>29.2</td>\n",
       "      <td>107.00</td>\n",
       "      <td>187</td>\n",
       "      <td>139.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.3820</td>\n",
       "      <td>95</td>\n",
       "      <td>244</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>29.2</td>\n",
       "      <td>93.00</td>\n",
       "      <td>249</td>\n",
       "      <td>174.2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.0039</td>\n",
       "      <td>92</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>93.00</td>\n",
       "      <td>253</td>\n",
       "      <td>181.2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.5433</td>\n",
       "      <td>98</td>\n",
       "      <td>230</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>36.6</td>\n",
       "      <td>113.00</td>\n",
       "      <td>199</td>\n",
       "      <td>94.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.63</td>\n",
       "      <td>5.7301</td>\n",
       "      <td>97</td>\n",
       "      <td>258</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>188</td>\n",
       "      <td>107.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.9703</td>\n",
       "      <td>73</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>28.3</td>\n",
       "      <td>101.00</td>\n",
       "      <td>179</td>\n",
       "      <td>107.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.7875</td>\n",
       "      <td>101</td>\n",
       "      <td>281</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE  SEX   BMI      BP   S1     S2    S3    S4      S5   S6    Y  Class\n",
       "134   28    1  30.4   85.00  198  115.6  67.0  3.00  4.3438   80  103      4\n",
       "17    68    2  27.5  111.00  214  147.0  39.0  5.00  4.9416   91  144      6\n",
       "23    61    2  32.0  103.67  210   85.2  35.0  6.00  6.1070  124  245      9\n",
       "39    48    2  27.7   73.00  191  119.4  46.0  4.00  4.8520   92   90      3\n",
       "330   51    2  29.2  107.00  187  139.0  32.0  6.00  4.3820   95  244      9\n",
       "..   ...  ...   ...     ...  ...    ...   ...   ...     ...  ...  ...    ...\n",
       "320   42    1  29.2   93.00  249  174.2  45.0  6.00  5.0039   92  122      5\n",
       "282   68    1  25.9   93.00  253  181.2  53.0  5.00  4.5433   98  230      8\n",
       "114   55    1  36.6  113.00  199   94.4  43.0  4.63  5.7301   97  258      9\n",
       "370   54    1  21.0   78.00  188  107.4  70.0  3.00  3.9703   73   63      2\n",
       "360   53    1  28.3  101.00  179  107.0  48.0  4.00  4.7875  101  281     10\n",
       "\n",
       "[89 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Q 1-4\n",
    "\n",
    "# initialize dataset\n",
    "data = pd.read_csv(\"C:/Users/haunt/Downloads/diabetes.csv\", sep=\"\\t\")\n",
    "sorted_data = data.sort_values(by='Y', ascending=False)\n",
    "sorted_data['Class'] = pd.qcut(sorted_data['Y'], q=10, labels=False, duplicates='raise') +1 # add Class col\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(sorted_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cd3c689-14fc-4343-b50d-05c04694f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 28.0000,   2.0000,  24.7000,  97.0000, 175.0000,  99.6000,  32.0000,\n",
      "           5.0000,   5.3799,  87.0000, 101.0000],\n",
      "        [ 49.0000,   2.0000,  28.8000,  92.0000, 207.0000, 140.0000,  44.0000,\n",
      "           5.0000,   4.7449,  92.0000, 196.0000],\n",
      "        [ 53.0000,   2.0000,  26.6000,  93.0000, 185.0000, 122.4000,  36.0000,\n",
      "           5.0000,   4.8903,  82.0000, 245.0000],\n",
      "        [ 44.0000,   1.0000,  25.1000, 133.0000, 182.0000, 113.0000,  55.0000,\n",
      "           3.0000,   4.2485,  84.0000, 216.0000],\n",
      "        [ 44.0000,   1.0000,  23.1000,  87.0000, 213.0000, 126.4000,  77.0000,\n",
      "           3.0000,   3.8712,  72.0000,  52.0000],\n",
      "        [ 54.0000,   1.0000,  22.6000,  90.0000, 183.0000, 104.2000,  64.0000,\n",
      "           3.0000,   4.3041,  92.0000,  72.0000],\n",
      "        [ 34.0000,   1.0000,  31.4000,  87.0000, 149.0000,  93.8000,  46.0000,\n",
      "           3.0000,   3.8286,  77.0000, 142.0000],\n",
      "        [ 54.0000,   1.0000,  24.2000,  74.0000, 204.0000, 109.0000,  82.0000,\n",
      "           2.0000,   4.1744, 109.0000,  92.0000],\n",
      "        [ 38.0000,   1.0000,  21.3000,  72.0000, 165.0000,  60.2000,  88.0000,\n",
      "           2.0000,   4.4308,  90.0000,  60.0000],\n",
      "        [ 60.0000,   1.0000,  25.6000,  87.0000, 207.0000, 125.8000,  69.0000,\n",
      "           3.0000,   4.1109,  84.0000,  81.0000]]) tensor([4, 7, 9, 8, 1, 2, 6, 3, 1, 3])\n",
      " epoch = 0, loss = 2.323301315307617, acc = 0.10370370745658875\n",
      " epoch = 100, loss = 2.221726894378662, acc = 0.2499999701976776\n",
      " epoch = 200, loss = 1.7009457349777222, acc = 0.2916666567325592\n",
      " epoch = 300, loss = 1.54225754737854, acc = 0.29722222685813904\n",
      " epoch = 400, loss = 1.3878958225250244, acc = 0.42129629850387573\n",
      " epoch = 500, loss = 1.3492679595947266, acc = 0.4175926148891449\n",
      " epoch = 600, loss = 1.2541790008544922, acc = 0.5074074268341064\n",
      " epoch = 700, loss = 1.2815965414047241, acc = 0.460185170173645\n",
      " epoch = 800, loss = 1.2737990617752075, acc = 0.4583333432674408\n",
      " epoch = 900, loss = 1.2138816118240356, acc = 0.46481481194496155\n",
      " epoch = 1000, loss = 1.285199522972107, acc = 0.40092596411705017\n",
      "loss = 1.2451938390731812, acc = 0.43611109256744385\n"
     ]
    }
   ],
   "source": [
    "# Q 5\n",
    "num_of_data_columns = 11\n",
    "class DiseaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.df=torch.tensor(data.to_numpy()).float()[:,:num_of_data_columns]\n",
    "        self.labels=torch.tensor(data['Class'].to_numpy().reshape(-1)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.df[index],self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "# Q 6-7\n",
    "dataset = DiseaseDataset(train_data)\n",
    "train_dataloader = DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "# print one batch\n",
    "dataiter = iter(train_dataloader)\n",
    "one_batch = next(dataiter)\n",
    "features,labels = one_batch\n",
    "print(features,labels)\n",
    "\n",
    "# Q 8 - neural network with Y as input\n",
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    " nn.Linear(11,30),\n",
    " nn.Tanh(),\n",
    " nn.Linear(30,30),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(30,20),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(20,10),\n",
    " nn.LogSoftmax(dim=1)\n",
    " )\n",
    "\n",
    "# trainning the neural network\n",
    "CE_loss = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def iterate_batch(features, labels):\n",
    "    labels = labels -1 \n",
    "    optimizer.zero_grad()\n",
    "    y_model = model(features)\n",
    "    loss = CE_loss(y_model,labels)\n",
    "    y_model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    predicted_labels = y_model.argmax(dim=1)\n",
    "    acc = (predicted_labels == labels).sum()/len(labels)\n",
    "    return loss.detach(), acc.detach()\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "loss = torch.zeros(batches)\n",
    "acc = torch.zeros(batches)\n",
    "\n",
    "epochs = 1001\n",
    "for i in range(epochs):\n",
    "    for j, (features, labels) in enumerate(train_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)       \n",
    "    if i % 100 == 0:\n",
    "        print(f\" epoch = {i}, loss = {loss.mean()}, acc = {acc.mean()}\")\n",
    "        \n",
    "# Q 10\n",
    "\n",
    "# testing the net on data test\n",
    "dataset = DiseaseDataset(test_data)\n",
    "test_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for j, (features, labels) in enumerate(test_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "print(f\"loss = {loss.mean()}, acc = {acc.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d6efb7-de94-4d0a-961c-ac7156cf3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch = 0, loss = 2.345939874649048, acc = 0.08611111342906952\n",
      " epoch = 100, loss = 2.3008675575256348, acc = 0.11759258806705475\n",
      " epoch = 200, loss = 2.3001346588134766, acc = 0.1111111119389534\n",
      " epoch = 300, loss = 2.297569751739502, acc = 0.1111111119389534\n",
      " epoch = 400, loss = 2.2862982749938965, acc = 0.11388888955116272\n",
      " epoch = 500, loss = 2.2291553020477295, acc = 0.15648148953914642\n",
      " epoch = 600, loss = 2.2236416339874268, acc = 0.15833333134651184\n",
      " epoch = 700, loss = 2.2798190116882324, acc = 0.10925926268100739\n",
      " epoch = 800, loss = 2.2910966873168945, acc = 0.12314815074205399\n",
      " epoch = 900, loss = 2.291245937347412, acc = 0.12870371341705322\n",
      " epoch = 1000, loss = 2.291400909423828, acc = 0.12222222238779068\n",
      "loss = 2.296696662902832, acc = 0.10864197462797165\n"
     ]
    }
   ],
   "source": [
    "# Ajustments for Q 9\n",
    "num_of_data_columns = 10 # without Y column\n",
    "class DiseaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.df=torch.tensor(data.to_numpy()).float()[:,:num_of_data_columns]\n",
    "        self.labels=torch.tensor(data['Class'].to_numpy().reshape(-1)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.df[index],self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "dataset = DiseaseDataset(train_data)\n",
    "train_dataloader = DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "\n",
    "# Q 9 - neural network without Y as input\n",
    "\n",
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    " nn.Linear(10,30),\n",
    " nn.Tanh(),\n",
    " nn.Linear(30,30),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(30,20),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(20,10),\n",
    " nn.LogSoftmax(dim=1)\n",
    " )\n",
    "\n",
    "CE_loss = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def iterate_batch(features, labels):\n",
    "    labels = labels -1\n",
    "    optimizer.zero_grad()\n",
    "    y_model = model(features)\n",
    "    loss = CE_loss(y_model,labels)\n",
    "    y_model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    predicted_labels = y_model.argmax(dim=1)\n",
    "    acc = (predicted_labels == labels).sum()/len(labels)\n",
    "    return loss.detach(), acc.detach()\n",
    "\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "loss = torch.zeros(batches)\n",
    "acc = torch.zeros(batches)\n",
    "\n",
    "epochs = 1001\n",
    "for i in range(epochs):\n",
    "    for j, (features, labels) in enumerate(train_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "    if i % 100 == 0:\n",
    "        print(f\" epoch = {i}, loss = {loss.mean()}, acc = {acc.mean()}\")\n",
    "        \n",
    "# Q 10\n",
    "\n",
    "dataset = DiseaseDataset(test_data)\n",
    "test_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for j, (features, labels) in enumerate(test_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "print(f\"loss = {loss.mean()}, acc = {acc.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ccead9-d701-43ee-a37d-ae1ceba8bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q 11:\n",
    "\n",
    "The net that includes Y col has more inputs that helps her to predict correctly the targets.\n",
    "Also the 'Y' col is the most important input, because the Class col depends mostly on Y col.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "686a9258-2b5f-4bbe-9810-13b0e99b6758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch = 0, loss = 4.660356521606445, acc = 0.011111111380159855\n",
      " epoch = 100, loss = 4.539266109466553, acc = 0.013888888992369175\n",
      " epoch = 200, loss = 4.530793190002441, acc = 0.01944444328546524\n",
      " epoch = 300, loss = 4.4937920570373535, acc = 0.03611110895872116\n",
      " epoch = 400, loss = 4.332884311676025, acc = 0.03888889029622078\n",
      " epoch = 500, loss = 4.119540691375732, acc = 0.0416666716337204\n",
      " epoch = 600, loss = 4.014353275299072, acc = 0.0555555559694767\n",
      " epoch = 700, loss = 3.9373881816864014, acc = 0.04722222313284874\n",
      " epoch = 800, loss = 3.872384786605835, acc = 0.06111111119389534\n",
      " epoch = 900, loss = 3.8195486068725586, acc = 0.07037036120891571\n",
      " epoch = 1000, loss = 3.7832934856414795, acc = 0.06388887763023376\n",
      "loss = 3.8555679321289062, acc = 0.0555555559694767\n"
     ]
    }
   ],
   "source": [
    "# Q 12\n",
    "sorted_data = data.sort_values(by='Y', ascending=False)\n",
    "sorted_data['Class'] = pd.qcut(sorted_data['Y'], q=100, labels=False, duplicates='raise') +1 # add Class col\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(sorted_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "num_of_data_columns = 11 # with Y col\n",
    "class DiseaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.df=torch.tensor(data.to_numpy()).float()[:,:num_of_data_columns]\n",
    "        self.labels=torch.tensor(data['Class'].to_numpy().reshape(-1)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.df[index],self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "\n",
    "dataset = DiseaseDataset(train_data)\n",
    "train_dataloader = DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "# neural network with Y as input\n",
    "\n",
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    " nn.Linear(11,30),\n",
    " nn.Tanh(),\n",
    " nn.Linear(30,30),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(30,20),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(20,100),\n",
    " nn.LogSoftmax(dim=1)\n",
    " )\n",
    "\n",
    "CE_loss = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def iterate_batch(features, labels):\n",
    "    labels = labels -1 \n",
    "    optimizer.zero_grad()\n",
    "    y_model = model(features)\n",
    "    loss = CE_loss(y_model,labels)\n",
    "    y_model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    predicted_labels = y_model.argmax(dim=1)\n",
    "    acc = (predicted_labels == labels).sum()/len(labels)\n",
    "    return loss.detach(), acc.detach()\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "loss = torch.zeros(batches)\n",
    "acc = torch.zeros(batches)\n",
    "\n",
    "epochs = 1001\n",
    "for i in range(epochs):\n",
    "    for j, (features, labels) in enumerate(train_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)       \n",
    "    if i % 100 == 0:\n",
    "        print(f\" epoch = {i}, loss = {loss.mean()}, acc = {acc.mean()}\")\n",
    "        \n",
    "# Q 10\n",
    "\n",
    "dataset = DiseaseDataset(test_data)\n",
    "test_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for j, (features, labels) in enumerate(test_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "print(f\"loss = {loss.mean()}, acc = {acc.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dced11ff-5e0a-491f-8999-5a6f3158644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch = 0, loss = 4.652468204498291, acc = 0.011111111380159855\n",
      " epoch = 100, loss = 4.535065650939941, acc = 0.01666666753590107\n",
      " epoch = 200, loss = 4.5302839279174805, acc = 0.01944444514811039\n",
      " epoch = 300, loss = 4.532470226287842, acc = 0.01944444328546524\n",
      " epoch = 400, loss = 4.533999919891357, acc = 0.01944444514811039\n",
      " epoch = 500, loss = 4.530945777893066, acc = 0.01944444514811039\n",
      " epoch = 600, loss = 4.528755187988281, acc = 0.01666666753590107\n",
      " epoch = 700, loss = 4.525008201599121, acc = 0.01944444514811039\n",
      " epoch = 800, loss = 4.522512435913086, acc = 0.01944444514811039\n",
      " epoch = 900, loss = 4.482946395874023, acc = 0.02222222276031971\n",
      " epoch = 1000, loss = 4.437296390533447, acc = 0.03611111640930176\n",
      "loss = 4.510091304779053, acc = 0.03333333507180214\n"
     ]
    }
   ],
   "source": [
    "num_of_data_columns = 10\n",
    "class DiseaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.df=torch.tensor(data.to_numpy()).float()[:,:num_of_data_columns]\n",
    "        self.labels=torch.tensor(data['Class'].to_numpy().reshape(-1)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.df[index],self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "\n",
    "dataset = DiseaseDataset(train_data)\n",
    "train_dataloader = DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "# neural network without Y as input\n",
    "\n",
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    " nn.Linear(10,30),\n",
    " nn.Tanh(),\n",
    " nn.Linear(30,30),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(30,20),\n",
    " nn.Sigmoid(),\n",
    " nn.Linear(20,100),\n",
    " nn.LogSoftmax(dim=1)\n",
    " )\n",
    "\n",
    "CE_loss = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def iterate_batch(features, labels):\n",
    "    labels = labels -1\n",
    "    optimizer.zero_grad()\n",
    "    y_model = model(features)\n",
    "    loss = CE_loss(y_model,labels)\n",
    "    y_model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    predicted_labels = y_model.argmax(dim=1)\n",
    "    acc = (predicted_labels == labels).sum()/len(labels)\n",
    "    return loss.detach(), acc.detach()\n",
    "\n",
    "batches = len(train_dataloader)\n",
    "loss = torch.zeros(batches)\n",
    "acc = torch.zeros(batches)\n",
    "\n",
    "epochs = 1001\n",
    "for i in range(epochs):\n",
    "    for j, (features, labels) in enumerate(train_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "    if i % 100 == 0:\n",
    "        print(f\" epoch = {i}, loss = {loss.mean()}, acc = {acc.mean()}\")\n",
    "        \n",
    "# Q 10\n",
    "\n",
    "dataset = DiseaseDataset(test_data)\n",
    "test_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for j, (features, labels) in enumerate(test_dataloader):\n",
    "        loss[j], acc[j] = iterate_batch(features, labels)\n",
    "print(f\"loss = {loss.mean()}, acc = {acc.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6665575-9a2e-4cad-9adf-a01ff79867af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q 14:\n",
    "\n",
    "I prefer 10 classes instead of 100 classes. \n",
    "One reason is that is much harder to distiguish between 100 classes than 10 classes'\n",
    "therefore the loss is higher in the 100 classes version.\n",
    "Second reason is that if I want to adjust my net to 100 outputs, I need much more neurons,\n",
    "and it will make the time run of my code much longer.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
